{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from patchify import patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(4, 4, 1, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "image = cv.imread(\"./Final Dataset/train/River/River_1.jpg\")\n",
    "image = np.asarray(image)\n",
    "print(image.shape)\n",
    "patches = patchify(image,(16,16,3),step=16)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(4, 4, 1, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "image_dataset = []\n",
    "\n",
    "size_x = (image.shape[1]//image_patch_size)*image_patch_size\n",
    "size_y = (image.shape[0]//image_patch_size)*image_patch_size\n",
    "image = Image.fromarray(image)\n",
    "print(image.show())\n",
    "image = image.crop((0,0, size_x, size_y))\n",
    "image = np.array(image)\n",
    "patched_images = patchify(image, (image_patch_size, image_patch_size, 3), step=image_patch_size)\n",
    "print(patched_images.shape)\n",
    "for i in range(patched_images.shape[0]):\n",
    "  for j in range(patched_images.shape[1]):\n",
    "    individual_patched_image = patched_images[i,j,:,:]\n",
    "    individual_patched_image = individual_patched_image[0]\n",
    "    image_dataset.append(individual_patched_image)\n",
    "    image = Image.fromarray(individual_patched_image)\n",
    "    print(image.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def generate_positive_patches(image, object_bbox, patch_size=64, iou_threshold=0.5):\n",
    "    patches = []\n",
    "    for bbox in object_bbox:\n",
    "        x, y, w, h = bbox\n",
    "        # Crop patch around the object\n",
    "        patch = TF.crop(image, y, x, h, w)\n",
    "        # Compute IoU\n",
    "        intersection = torch.tensor([max(0, min(x+w, x2+w2) - max(x, x2)) * max(0, min(y+h, y2+h2) - max(y, y2)) for x2, y2, w2, h2 in object_bbox])\n",
    "        union = w * h + torch.tensor([(w2 * h2) for _, _, w2, h2 in object_bbox]) - intersection\n",
    "        iou = intersection / union\n",
    "        # Label patch based on IoU\n",
    "        label = torch.tensor([1 if iou_val >= iou_threshold else 0 for iou_val in iou])\n",
    "        patches.append((patch, label))\n",
    "    return patches\n",
    "\n",
    "def generate_negative_patches(image, object_bbox, num_patches=5, patch_size=64):\n",
    "    patches = []\n",
    "    height, width = image.shape[-2:]\n",
    "    for _ in range(num_patches):\n",
    "        # Randomly crop patch from background\n",
    "        x = torch.randint(0, width - patch_size, (1,))\n",
    "        y = torch.randint(0, height - patch_size, (1,))\n",
    "        patch = TF.crop(image, int(y), int(x), patch_size, patch_size)\n",
    "        patches.append((patch, torch.tensor([0])))\n",
    "    return patches\n",
    "\n",
    "# Example usage\n",
    "image = torch.rand(3, 256, 256)  # Example image tensor (3 channels, 256x256)\n",
    "object_bbox = [(50, 50, 100, 100)]  # Example object bounding box (x, y, width, height)\n",
    "\n",
    "positive_patches = generate_positive_patches(image, object_bbox)\n",
    "negative_patches = generate_negative_patches(image, object_bbox)\n",
    "\n",
    "# Now you have positive_patches and negative_patches with corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m object_bbox \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m)]  \u001b[38;5;66;03m# Example object bounding box (x, y, width, height)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m positive_patches \u001b[38;5;241m=\u001b[39m generate_positive_patches(image, object_bbox)\n\u001b[1;32m---> 17\u001b[0m negative_patches \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_negative_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_bbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplaying Positive Patches:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m display_patches(positive_patches)\n",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m, in \u001b[0;36mgenerate_negative_patches\u001b[1;34m(image, object_bbox, num_patches, patch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_negative_patches\u001b[39m(image, object_bbox, num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m     20\u001b[0m     patches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_patches):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Randomly crop patch from background\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, width \u001b[38;5;241m-\u001b[39m patch_size, (\u001b[38;5;241m1\u001b[39m,))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_patches(patches):\n",
    "    for i, (patch, _) in enumerate(patches):\n",
    "        patch_image = to_pil_image(patch)\n",
    "        patch_image.show()\n",
    "\n",
    "# Example usage\n",
    "image = cv.imread(\"./Final Dataset/train/River/River_1.jpg\") # Example image tensor (3 channels, 256x256)\n",
    "image = Image.fromarray(image)\n",
    "object_bbox = [(50, 50, 100, 100)]  # Example object bounding box (x, y, width, height)\n",
    "\n",
    "positive_patches = generate_positive_patches(image, object_bbox)\n",
    "negative_patches = generate_negative_patches(image, object_bbox)\n",
    "\n",
    "print(\"Displaying Positive Patches:\")\n",
    "display_patches(positive_patches)\n",
    "\n",
    "print(\"Displaying Negative Patches:\")\n",
    "display_patches(negative_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
